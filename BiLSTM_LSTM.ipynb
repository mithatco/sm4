{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "tjjDMp6K0TIz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install torchtext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITpQEMkN7hJ0",
        "outputId": "1b0ff337-557e-4cff-aac4-7c2181fb86db"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext\n",
            "  Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.32.3)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.4.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (2024.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.3.0->torchtext) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.3.0->torchtext) (1.3.0)\n",
            "Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchtext\n",
            "Successfully installed torchtext-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNM47ONe3rmo",
        "outputId": "62b63ffa-a457-412f-c7e1-72501eee7366"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
        "\n",
        "\n",
        "def build_vocab(train_texts):\n",
        "    vocab = Counter()\n",
        "    for text in train_texts:\n",
        "        tokens = tokenize(text)\n",
        "        vocab.update(tokens)\n",
        "\n",
        "    # word-to-index dictionary with special <PAD> token as 0\n",
        "    word2idx = {word: idx+1 for idx, (word, _) in enumerate(vocab.most_common())}\n",
        "    word2idx['<PAD>'] = 0\n",
        "    return word2idx\n"
      ],
      "metadata": {
        "id": "adh490Vs7yHS"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TextDataset Class\n",
        "class TweetDataset(Dataset):\n",
        "    def __init__(self, texts, labels, vocab):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.vocab = vocab\n",
        "        self.tokenizer = tokenize\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tokens = self.tokenizer(self.texts[idx])\n",
        "        indices = [self.vocab[token] if token in self.vocab else 0 for token in tokens]\n",
        "        return torch.tensor(indices, dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "W-byxNAG2Nem"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# collate function for padding sequences in DataLoader\n",
        "def collate_fn(batch):\n",
        "    texts, labels = zip(*batch)\n",
        "    padded_texts = nn.utils.rnn.pad_sequence(texts, batch_first=True, padding_value=0)\n",
        "    return padded_texts, torch.tensor(labels)\n"
      ],
      "metadata": {
        "id": "HF8lZH1f768F"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BiLSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(BiLSTMModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim  # Store hidden_dim in the class instance\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "\n",
        "        hidden = torch.cat((lstm_out[:, -1, :self.hidden_dim], lstm_out[:, 0, self.hidden_dim:]), dim=1)\n",
        "        output = self.fc(self.dropout(hidden))\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "yzwk0F7z2TGK"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim  # Store hidden_dim in the class instance\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)  # Non-bidirectional LSTM\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        hidden = lstm_out[:, -1, :]  # Take the output from the last time step\n",
        "        output = self.fc(self.dropout(hidden))\n",
        "        return output"
      ],
      "metadata": {
        "id": "BMjY-UecG-o0"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train and evaluate model\n",
        "def train_and_evaluate_model(train_loader, test_loader, model, optimizer, criterion, epochs=20):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        for texts, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(texts)\n",
        "            loss = criterion(predictions, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Loss: {epoch_loss/len(train_loader)}')\n",
        "\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in test_loader:\n",
        "            predictions = model(texts).argmax(1)\n",
        "            all_preds.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    print(f\"Accuracy: {accuracy_score(all_labels, all_preds)}\")\n",
        "    print(classification_report(all_labels, all_preds))\n"
      ],
      "metadata": {
        "id": "qDIKZMd22X_u"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # datasets\n",
        "    olid_train = pd.read_csv(\"/content/drive/MyDrive/datasets/olid-train-small.csv\")\n",
        "    olid_test = pd.read_csv(\"/content/drive/MyDrive/datasets/olid-test.csv\")\n",
        "    hasoc_train = pd.read_csv(\"/content/drive/MyDrive/datasets/hasoc-train.csv\")\n",
        "\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    olid_train['labels'] = le.fit_transform(olid_train['labels'])\n",
        "    olid_test['labels'] = le.transform(olid_test['labels'])\n",
        "    hasoc_train['labels'] = le.transform(hasoc_train['labels'])\n",
        "\n",
        "    # vocabulary from the OLID training set\n",
        "    vocab = build_vocab(olid_train['text'].values)\n",
        "    vocab_size = len(vocab) + 1  # Adding 1 for padding token\n",
        "\n",
        "    train_dataset_olid = TweetDataset(olid_train['text'].values, olid_train['labels'].values, vocab)\n",
        "    test_dataset_olid = TweetDataset(olid_test['text'].values, olid_test['labels'].values, vocab)\n",
        "    train_dataset_hasoc = TweetDataset(hasoc_train['text'].values, hasoc_train['labels'].values, vocab)\n",
        "\n",
        "    train_loader_olid = DataLoader(train_dataset_olid, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "    test_loader_olid = DataLoader(test_dataset_olid, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "    train_loader_hasoc = DataLoader(train_dataset_hasoc, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "    embedding_dim = 100\n",
        "    hidden_dim = 128\n",
        "    output_dim = 2\n",
        "\n",
        "    # BiLSTM Model\n",
        "    bilstm_model = BiLSTMModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
        "    optimizer_bilstm = optim.Adam(bilstm_model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(\"Training BiLSTM model on OLIDv1 small dataset and evaluating on OLIDv1 test set:\")\n",
        "    train_and_evaluate_model(train_loader_olid, test_loader_olid, bilstm_model, optimizer_bilstm, criterion)\n",
        "\n",
        "    # LSTM Model\n",
        "    lstm_model = LSTMModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
        "    optimizer_lstm = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
        "\n",
        "    print(\"Training LSTM model on OLIDv1 small dataset and evaluating on OLIDv1 test set:\")\n",
        "    train_and_evaluate_model(train_loader_olid, test_loader_olid, lstm_model, optimizer_lstm, criterion)\n",
        "\n",
        "    print(\"Training BiLSTM model on HASOC dataset and evaluating on OLIDv1 test set:\")\n",
        "    train_and_evaluate_model(train_loader_hasoc, test_loader_olid, bilstm_model, optimizer_bilstm, criterion)\n",
        "\n",
        "    print(\"Training LSTM model on HASOC dataset and evaluating on OLIDv1 test set:\")\n",
        "    train_and_evaluate_model(train_loader_hasoc, test_loader_olid, lstm_model, optimizer_lstm, criterion)\n"
      ],
      "metadata": {
        "id": "w2Wb6aH-2bMd"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbtdGXdo2ebp",
        "outputId": "85848825-7e90-4431-fc3f-b52aabc44467"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training BiLSTM model on OLIDv1 small dataset and evaluating on OLIDv1 test set:\n",
            "Epoch 1, Loss: 0.6653532105716852\n",
            "Epoch 2, Loss: 0.6119593300454603\n",
            "Epoch 3, Loss: 0.5248462912814865\n",
            "Epoch 4, Loss: 0.43427793942188303\n",
            "Epoch 5, Loss: 0.32324328701027105\n",
            "Epoch 6, Loss: 0.22842486587459923\n",
            "Epoch 7, Loss: 0.15070984824026218\n",
            "Epoch 8, Loss: 0.09470858477320665\n",
            "Epoch 9, Loss: 0.053557778404557395\n",
            "Epoch 10, Loss: 0.04272946701868554\n",
            "Epoch 11, Loss: 0.027798506840643642\n",
            "Epoch 12, Loss: 0.02507715368079775\n",
            "Epoch 13, Loss: 0.03345857605165064\n",
            "Epoch 14, Loss: 0.02712121066831999\n",
            "Epoch 15, Loss: 0.016828030867558075\n",
            "Epoch 16, Loss: 0.027366643499575374\n",
            "Epoch 17, Loss: 0.02054120414086256\n",
            "Epoch 18, Loss: 0.011223362209321486\n",
            "Epoch 19, Loss: 0.005362710950812173\n",
            "Epoch 20, Loss: 0.0048589571401986735\n",
            "Accuracy: 0.708139534883721\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.76      0.79       620\n",
            "           1       0.48      0.58      0.53       240\n",
            "\n",
            "    accuracy                           0.71       860\n",
            "   macro avg       0.65      0.67      0.66       860\n",
            "weighted avg       0.73      0.71      0.72       860\n",
            "\n",
            "Training LSTM model on OLIDv1 small dataset and evaluating on OLIDv1 test set:\n",
            "Epoch 1, Loss: 0.6696395512487068\n",
            "Epoch 2, Loss: 0.6651393797228246\n",
            "Epoch 3, Loss: 0.6645299821603493\n",
            "Epoch 4, Loss: 0.6627762319286012\n",
            "Epoch 5, Loss: 0.6617453450062236\n",
            "Epoch 6, Loss: 0.6593782976676857\n",
            "Epoch 7, Loss: 0.653883425590119\n",
            "Epoch 8, Loss: 0.653038095255367\n",
            "Epoch 9, Loss: 0.650473049103888\n",
            "Epoch 10, Loss: 0.6381971054389829\n",
            "Epoch 11, Loss: 0.6493749540360247\n",
            "Epoch 12, Loss: 0.6416678102941461\n",
            "Epoch 13, Loss: 0.6380447152533818\n",
            "Epoch 14, Loss: 0.6198264479311437\n",
            "Epoch 15, Loss: 0.5493132265213408\n",
            "Epoch 16, Loss: 0.4758118501777857\n",
            "Epoch 17, Loss: 0.391617074038813\n",
            "Epoch 18, Loss: 0.3339723261490546\n",
            "Epoch 19, Loss: 0.31130166978783946\n",
            "Epoch 20, Loss: 0.28764372115415304\n",
            "Accuracy: 0.7186046511627907\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.82      0.81       620\n",
            "           1       0.50      0.47      0.48       240\n",
            "\n",
            "    accuracy                           0.72       860\n",
            "   macro avg       0.65      0.64      0.64       860\n",
            "weighted avg       0.71      0.72      0.72       860\n",
            "\n",
            "Training BiLSTM model on HASOC dataset and evaluating on OLIDv1 test set:\n",
            "Epoch 1, Loss: 0.7462033935583354\n",
            "Epoch 2, Loss: 0.5763498225498721\n",
            "Epoch 3, Loss: 0.5120818828298745\n",
            "Epoch 4, Loss: 0.43124514541339354\n",
            "Epoch 5, Loss: 0.3338276665718829\n",
            "Epoch 6, Loss: 0.2430383435201124\n",
            "Epoch 7, Loss: 0.17152594571393695\n",
            "Epoch 8, Loss: 0.12514569213400123\n",
            "Epoch 9, Loss: 0.08807743425720213\n",
            "Epoch 10, Loss: 0.06629509430460102\n",
            "Epoch 11, Loss: 0.05328101225075175\n",
            "Epoch 12, Loss: 0.04540954849776899\n",
            "Epoch 13, Loss: 0.0737642851541114\n",
            "Epoch 14, Loss: 0.06879300942970186\n",
            "Epoch 15, Loss: 0.044511339147549754\n",
            "Epoch 16, Loss: 0.0396868147075563\n",
            "Epoch 17, Loss: 0.03654172794850126\n",
            "Epoch 18, Loss: 0.03141771966770872\n",
            "Epoch 19, Loss: 0.029598028818272625\n",
            "Epoch 20, Loss: 0.027152725531022406\n",
            "Accuracy: 0.6558139534883721\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.70      0.75       620\n",
            "           1       0.41      0.54      0.47       240\n",
            "\n",
            "    accuracy                           0.66       860\n",
            "   macro avg       0.60      0.62      0.61       860\n",
            "weighted avg       0.69      0.66      0.67       860\n",
            "\n",
            "Training LSTM model on HASOC dataset and evaluating on OLIDv1 test set:\n",
            "Epoch 1, Loss: 0.6707259620147976\n",
            "Epoch 2, Loss: 0.6314362437347245\n",
            "Epoch 3, Loss: 0.6062933141416539\n",
            "Epoch 4, Loss: 0.5778486769381768\n",
            "Epoch 5, Loss: 0.5375252357923268\n",
            "Epoch 6, Loss: 0.49983906509772025\n",
            "Epoch 7, Loss: 0.4725829506180977\n",
            "Epoch 8, Loss: 0.4537234829097498\n",
            "Epoch 9, Loss: 0.43156887607170585\n",
            "Epoch 10, Loss: 0.41770245501252473\n",
            "Epoch 11, Loss: 0.4080298105712797\n",
            "Epoch 12, Loss: 0.3857177518756012\n",
            "Epoch 13, Loss: 0.3651537887467061\n",
            "Epoch 14, Loss: 0.3586809220698362\n",
            "Epoch 15, Loss: 0.3363689477733576\n",
            "Epoch 16, Loss: 0.3311718076141806\n",
            "Epoch 17, Loss: 0.30932344329324574\n",
            "Epoch 18, Loss: 0.2902473909118788\n",
            "Epoch 19, Loss: 0.2839627566396213\n",
            "Epoch 20, Loss: 0.2554294952277929\n",
            "Accuracy: 0.6848837209302325\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.77      0.78       620\n",
            "           1       0.44      0.47      0.46       240\n",
            "\n",
            "    accuracy                           0.68       860\n",
            "   macro avg       0.62      0.62      0.62       860\n",
            "weighted avg       0.69      0.68      0.69       860\n",
            "\n"
          ]
        }
      ]
    }
  ]
}